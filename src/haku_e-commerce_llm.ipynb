{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a2BfPX9Z8p1",
    "outputId": "c07720b5-c3fe-4800-d7ae-80af069f4242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "Requirement already satisfied: gradio in /opt/conda/lib/python3.10/site-packages (4.44.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.4.5)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: fastapi<1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.115.0)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.0.10)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.30.5)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.3.0)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.10.7)\n",
      "Requirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.6.8)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.3)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1.0->gradio) (0.37.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "Requirement already satisfied: pillow-avif-plugin in /opt/conda/lib/python3.10/site-packages (1.4.6)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (10.4.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U gradio \n",
    "!pip install pillow-avif-plugin Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaULqLbwaoGo",
    "outputId": "96382dc5-ee47-499a-8e0c-332a1f001be4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/jupyter/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import gradio as gr\n",
    "import time\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "from skimage import io  \n",
    "import shutil   \n",
    "import json  # Import json to work with JSON files\n",
    "import urllib.request as request  # Import urllib to make HTTP requests\n",
    "import warp\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p3mUxxAIb4hw"
   },
   "outputs": [],
   "source": [
    "HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iOcYTep2cMTW"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "7df5a9eef9e048a6978f8e3d729a5704",
      "89aa5ec9958343d9966d35581b21a5aa",
      "bbc1b4af122642d2842978091c40806c",
      "45a850cd644742eab40a243fd1c85d4c",
      "3185e496830042c3bfef1d0530b2c392",
      "e4a39eb9fb6c481ba08bcd9853210849",
      "f0062dc0f3b94a93bca682fdb7b130dc",
      "059243a6afb442ccbef314aebe1dc5ee",
      "0b4f1e8b5afd42c39ddd4dc803da0164",
      "a1aab42246d14a1db588e7abb1c4001f",
      "0ced19ed88d34b65b44dd6da8aa38de4"
     ]
    },
    "id": "CMVnNWzEZmH6",
    "outputId": "6bc2eba6-d9f3-4be7-d098-4ac549cb85d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302699f2ffb34a13a53d04b311055cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a271ef366e4f439bb5d7c57d2af02533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id_2 = \"Trendyol/Trendyol-LLM-7b-chat-v1.8\"\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_id_2,use_fast=False)\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(model_id_2, \n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype= torch.bfloat16,\n",
    "                                             load_in_8bit=True)\n",
    "\n",
    "sampling_params_2 = dict(do_sample=True, temperature=0.3, top_k=50, top_p=0.9)\n",
    "\n",
    "pipe_2 = pipeline(\"text-generation\", \n",
    "                model=model_2, \n",
    "                tokenizer=tokenizer_2,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype= torch.bfloat16,\n",
    "                max_new_tokens=256, \n",
    "                return_full_text=True,\n",
    "                repetition_penalty=0.8\n",
    "               )\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT_2 = \"Bir e ticaret sitesi i√ßin a√ßƒ±klama √ºreten bir yapay zeka modelisin \\n\"\n",
    "\n",
    "TEMPLATE_2 = (\n",
    "    \"[INST] {system_prompt}\\n\\n\"\n",
    "    \"{instruction} [/INST]\"\n",
    ")\n",
    "\n",
    "def generate_prompt_2(instruction, system_prompt=DEFAULT_SYSTEM_PROMPT_2):\n",
    "    return TEMPLATE_2.format_map({'instruction': instruction,'system_prompt': system_prompt})\n",
    "\n",
    "def generate_output_2(user_query, sys_prompt=DEFAULT_SYSTEM_PROMPT_2):\n",
    "    prompt = generate_prompt_2(user_query, sys_prompt)\n",
    "    outputs = pipe_2(prompt,\n",
    "               **sampling_params_2\n",
    "              )\n",
    "    return outputs[0][\"generated_text\"].split(\"[/INST]\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "iF5-JS1LtsbW",
    "outputId": "d8c02ebe-1e9e-40a7-ccf0-9916b6cccb9a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gradio/utils.py:1002: UserWarning: Expected 4 arguments for function <function submission_call at 0x7f95b85e0040>, received 3.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/gradio/utils.py:1006: UserWarning: Expected at least 4 arguments for function <function submission_call at 0x7f95b85e0040>, received 3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://f1f764eef83d0e7f06.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f1f764eef83d0e7f06.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gradio/helpers.py:978: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00001_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00002_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00002_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00002_.png\n",
      "Image found: ./ComfyUI/output/ComfyUI_00002_.png\n"
     ]
    }
   ],
   "source": [
    "def queue_prompt(prompt):\n",
    "    p = {\"prompt\": prompt}\n",
    "    data = json.dumps(p).encode('utf-8')\n",
    "    req = request.Request(\"http://127.0.0.1:3000/prompt\", data=data, headers={'Content-Type': 'application/json'})\n",
    "    response = request.urlopen(req)  # Make the HTTP request\n",
    "    print(response.read().decode())  # Print the response\n",
    "    \n",
    "def clear_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)  # Remove file\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove folder\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "                \n",
    "def process_input(input_text):\n",
    "    # Define the tags\n",
    "    header_start = \"<|start_header_id|>\"\n",
    "    header_end = \"<|end_header_id|>\"\n",
    "    eot_id = \"<|eot_id|>\"\n",
    "\n",
    "    # Find the last occurrence of the header tags\n",
    "    header_start_idx = input_text.rfind(header_start) + len(header_start)\n",
    "    header_end_idx = input_text.rfind(header_end)\n",
    "    bot_name = input_text[header_start_idx:header_end_idx].strip()\n",
    "\n",
    "    # Find the message content between the last end header and the eot tag\n",
    "    message_start_idx = header_end_idx + len(header_end)\n",
    "\n",
    "    # Check if the eot_id exists and if the string ends with it\n",
    "    if input_text.endswith(eot_id):\n",
    "        message_end_idx = input_text.rfind(eot_id)\n",
    "    else:\n",
    "        message_end_idx = len(input_text)\n",
    "\n",
    "    message = input_text[message_start_idx:message_end_idx].strip()\n",
    "\n",
    "    return f\"{message}\"\n",
    "\n",
    "def create_response(uploaded_image,content,max_new_tokens=200,process_input_boolean=True):\n",
    "    # Example input_text processing (replace with your actual processor code)\n",
    "    input_text = processor.apply_chat_template(content, add_generation_prompt=True) #works\n",
    "    inputs = processor(uploaded_image, input_text, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    bot_message = processor.decode(output[0])\n",
    "    if process_input_boolean:\n",
    "        bot_message = process_input(bot_message)\n",
    "    return bot_message\n",
    "\n",
    "def get_product_name(explanation,uploaded_image):\n",
    "    message = f\"\"\"Based on the provided product description and image: '{explanation}' explain the product at image.\"\"\"\n",
    "    content = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": message}\n",
    "    ]}\n",
    "    ]\n",
    "    name_answer = create_response(uploaded_image,content,max_new_tokens=100)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": name_answer}\n",
    "    ]})\n",
    "    message_2 = \"Create a proper name for the product.\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_2}\n",
    "    ]})\n",
    "    name_answer_2 = create_response(uploaded_image,content,max_new_tokens=30)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": name_answer_2}\n",
    "    ]})\n",
    "\n",
    "    return name_answer_2\n",
    "\n",
    "def get_target_audience(product_name,uploaded_image):\n",
    "    message = f\"\"\"Can you provide detailed information about {product_name} ?\"\"\"\n",
    "\n",
    "    content = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": message}\n",
    "    ]}\n",
    "    ]\n",
    "    target_answer = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": target_answer}\n",
    "    ]})\n",
    "    message_2 = f\"\"\"Based on the information about {product_name} and image, which type of people can be our target audience and why?\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_2}\n",
    "    ]})\n",
    "    target_answer_2 = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": target_answer_2}\n",
    "    ]})\n",
    "    message_3 = f\"\"\"I want one target audience that includes all of these.\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_3}\n",
    "    ]})\n",
    "    target_answer_3 = create_response(uploaded_image,content,max_new_tokens=50)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": target_answer_3}\n",
    "    ]})\n",
    "\n",
    "    return target_answer_3\n",
    "\n",
    "def get_strong_message(product_name,uploaded_image,target_audience,all_features):\n",
    "    message = f\"\"\"Please provide a comprehensive overview of {product_name} at the image. Do not add information that you are not sure.\"\"\"\n",
    "    content = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": message}\n",
    "    ]}\n",
    "    ]\n",
    "    sm_answer = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": sm_answer}\n",
    "    ]})\n",
    "    message_3 = f\"\"\"Create a e commerce explanation of {product_name} for {target_audience} using these features {all_features} and previous information. Do not make longer than 200 words.\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_3}\n",
    "    ]})\n",
    "    sm_answer_3 = create_response(uploaded_image,content)\n",
    "\n",
    "    return f\"{sm_answer_3} features:{all_features}\"\n",
    "\n",
    "def get_all_features(product_name, uploaded_image):\n",
    "    message = f\"\"\"Please provide details about features of {product_name} at the image.Do not add any feature that is visible at the image. \"\"\"\n",
    "    content = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": message}\n",
    "    ]}\n",
    "    ]\n",
    "    f_answer = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": f_answer}\n",
    "    ]})\n",
    "    message_2 = f\"\"\"Given the detailed information about {product_name} what is the features about the product at the image?Do not add any feature that is not provided.Do not add features about other objects. \"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_2}\n",
    "    ]})\n",
    "    f_answer_2 = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": f_answer_2}\n",
    "    ]})\n",
    "    message_3 = \"\"\"Create a dictionary for the features of the product in this format:{'color':'black',...}.Do not add any feature that you are not sure. Only provide the format.\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_3}\n",
    "    ]})\n",
    "    f_answer_3 = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": f_answer_3}\n",
    "    ]})\n",
    "    all_features = f_answer_3\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "def chat_call(message, chat_history, uploaded_image, result_text):\n",
    "    try:\n",
    "        # Eƒüer g√∂rsel veya a√ßƒ±klama eksikse\n",
    "        if uploaded_image is None or (result_text == \"\" and len(chat_history) == 0):\n",
    "            gr.Warning(\"√úr√ºn G√∂rseli ya da A√ßƒ±klama Eksik.\")\n",
    "        else:\n",
    "            history_openai_format = []\n",
    "            if len(chat_history)==0:\n",
    "                history_openai_format.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": 'Provide explanation for e commerce store'},  # √úr√ºn a√ßƒ±klamasƒ±\n",
    "                    ]\n",
    "                })\n",
    "                history_openai_format.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": result_text}]})\n",
    "                chat_history.append(('Provide explanation for e commerce store', result_text))\n",
    "            else:\n",
    "                history_openai_format.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": 'Provide explanation for e commerce store'},  # √úr√ºn a√ßƒ±klamasƒ±\n",
    "                    ]\n",
    "                })\n",
    "                history_openai_format.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": result_text}]})\n",
    "            if len(chat_history)>4:\n",
    "                recent_history = chat_history[-3:]\n",
    "                for human, assistant in recent_history:\n",
    "                    history_openai_format.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": human}]})\n",
    "                    history_openai_format.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant}]})\n",
    "                history_openai_format.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]})    \n",
    "                target_answer = create_response(uploaded_image, history_openai_format)\n",
    "                chat_history.append((message, target_answer))  # Kullanƒ±cƒ±nƒ±n mesajƒ±nƒ± chat_history'ye ekliyoruz\n",
    "            else:\n",
    "                for human, assistant in chat_history:\n",
    "                    history_openai_format.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": human}]})\n",
    "                    history_openai_format.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant}]})\n",
    "                history_openai_format.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]}) \n",
    "                target_answer = create_response(uploaded_image, history_openai_format) \n",
    "                chat_history.append((message, target_answer))  # Kullanƒ±cƒ±nƒ±n mesajƒ±nƒ± chat_history'ye ekliyoruz\n",
    "                \n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred: {str(e)}\"\n",
    "        chat_history.append((message, error_message))\n",
    "\n",
    "    return \"\", chat_history\n",
    "\n",
    "def submission_call(message, uploaded_image,chat_history,language):\n",
    "    try:\n",
    "        # If no image is uploaded\n",
    "        if uploaded_image is None or message == \"\":\n",
    "            gr.Warning(\"√úr√ºn G√∂rseli ya da A√ßƒ±klama Eksik.\")\n",
    "        else:\n",
    "            product_name = get_product_name(message, uploaded_image)\n",
    "            all_features = get_all_features(message, uploaded_image)\n",
    "            target_audience = get_target_audience(product_name,uploaded_image)\n",
    "            strong_message = get_strong_message(product_name,uploaded_image,target_audience,all_features)\n",
    "            return strong_message\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "def create_call(img_output, warp_boolean):\n",
    "    def queue_prompt(prompt):\n",
    "        p = {\"prompt\": prompt}\n",
    "        data = json.dumps(p).encode('utf-8')\n",
    "        req = request.Request(\"http://127.0.0.1:3000/prompt\", data=data, headers={'Content-Type': 'application/json'})\n",
    "        response = request.urlopen(req)  # Make the HTTP request\n",
    "    # Function to get the latest file from a directory\n",
    "    def get_latest_file_in_folder(folder_path):\n",
    "        files = os.listdir(folder_path)\n",
    "        if files:\n",
    "            # Get the latest file based on modification time\n",
    "            return sorted(files, key=lambda x: os.path.getmtime(os.path.join(folder_path, x)))[0]\n",
    "        return None\n",
    "    with open('gercek_api.json', 'r') as file:\n",
    "        prompt = json.load(file)  # Load JSON content from the file\n",
    "        # Get the latest file names in each folder\n",
    "    queue_prompt(prompt)\n",
    "    # Define the folder paths\n",
    "    main_folder = \"./ComfyUI/input/main\"\n",
    "    background_folder = \"./ComfyUI/input/background\"\n",
    "    \n",
    "    main_image = get_latest_file_in_folder(main_folder)  # Get the latest file from the main folder\n",
    "    background_image = get_latest_file_in_folder(background_folder)  # Get the latest file from the background folder\n",
    "    if warp_boolean:\n",
    "        warp.warp(f\"./ComfyUI/input/main/{main_image}\",f\"./ComfyUI/input/main/{main_image}\")\n",
    "    # Update the prompt dictionary with the relative paths of the files\n",
    "    if main_image is not None:\n",
    "        prompt['10']['inputs']['image'] = f\"main/{main_image}\"  # Update image input with the latest main folder file \n",
    "    else:\n",
    "        print(\"No image found in the main folder.\")\n",
    "\n",
    "    if background_image is not None:\n",
    "        prompt['39']['inputs']['image'] = f\"background/{background_image}\"  # Update background image input with only the background path\n",
    "    else:\n",
    "        print(\"No image found in the background folder.\")\n",
    "    queue_prompt(prompt)\n",
    "    OUTPUT_FOLDER = \"./ComfyUI/output\"\n",
    "    clear_folder(OUTPUT_FOLDER) \n",
    "def refresh(result_box):\n",
    "    OUTPUT_FOLDER = \"./ComfyUI/output\"\n",
    "    for file_name in os.listdir(OUTPUT_FOLDER):\n",
    "        # Check if the file is an image (you can add more extensions if needed)\n",
    "        if file_name.endswith(('.png', '.jpg', '.jpeg', '.webp', '.gif')):\n",
    "            image_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
    "            print(f\"Image found: {image_path}\")\n",
    "            return image_path\n",
    "    \n",
    "def upload_file(file):    \n",
    "    UPLOAD_FOLDER = \"./ComfyUI/input/main\" \n",
    "    clear_folder(UPLOAD_FOLDER) \n",
    "    if not os.path.exists(UPLOAD_FOLDER):    \n",
    "        os.mkdir(UPLOAD_FOLDER)    \n",
    "    shutil.copy(file, UPLOAD_FOLDER)    \n",
    "    gr.Info(\"Main image uploaded!!!\")\n",
    "    \n",
    "def upload_file_background(file):    \n",
    "    UPLOAD_FOLDER = \"./ComfyUI/input/background\"\n",
    "    clear_folder(UPLOAD_FOLDER) \n",
    "    if not os.path.exists(UPLOAD_FOLDER):    \n",
    "        os.mkdir(UPLOAD_FOLDER)    \n",
    "    shutil.copy(file, UPLOAD_FOLDER)    \n",
    "    gr.Info(\"Background uploaded!!!\")  \n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# E ticaret a√ßƒ±klama olu≈üturma\")\n",
    "    with gr.Column():\n",
    "        with gr.Row():  # Create a row to position components horizontally\n",
    "            # Left Panel\n",
    "            with gr.Column(scale=2):  # Image upload button on the left\n",
    "                img_input = gr.Image(label=\"√úr√ºn G√∂rseli Ekleyiniz.\")  # Image input component\n",
    "                chatbot = gr.Chatbot()\n",
    "                msg = gr.Textbox(placeholder=\"A√ßƒ±klamayƒ± deƒüi≈ütirmek i√ßin chatbot ile konu≈üabilirsin.\", label=\"Chatbot\")\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        chat_button = gr.Button(\"G√∂nder\")\n",
    "                    with gr.Column():\n",
    "                        # Clear button with a trash can emoji in the text\n",
    "                        clear = gr.Button(\"Sohbeti Temizle üóëÔ∏è\")\n",
    "\n",
    "            # Right Panel\n",
    "            with gr.Column(scale=3):\n",
    "                submission_text = gr.Textbox(placeholder=\"√úr√ºn A√ßƒ±klamasƒ±\", label=\"√úr√ºn√ºn√ºz hakkƒ±nda kƒ±sa bir a√ßƒ±klama giriniz.\")\n",
    "                submission_button = gr.Button(\"Olu≈ütur\")\n",
    "                with gr.Accordion('√áƒ±ktƒ±:', open=True):\n",
    "                    result_box = gr.Markdown(value='', label=\"\")\n",
    "\n",
    "\n",
    "                # Adding radio button for language selection\n",
    "                language_radio = gr.Radio(\n",
    "                    choices=[\"English\", \"T√ºrk√ße\"], \n",
    "                    label=\"Dil Se√ßimi\", \n",
    "                    value=\"English\"  # Default selected value\n",
    "                )\n",
    "        gr.Markdown(\"# G√∂r√ºnt√º i≈üleme\")\n",
    "        upload_button = gr.UploadButton(\"Upload main image\")   \n",
    "        upload_button_background = gr.UploadButton(\"Upload background image\")\n",
    "        create_button = gr.Button('Create new image')\n",
    "        img_output = gr.Image(type=\"filepath\", interactive=False, label=\"Output Image\")\n",
    "        warp_checkbox = gr.Checkbox(label=\"Warp Uygula\", value=False)  # Default unchecked\n",
    "        \n",
    "            \n",
    "    # Bind both the submit event on the text box and the button click to the chat_call function\n",
    "    msg.submit(chat_call, [msg, chatbot, img_input, result_box], [msg, chatbot])\n",
    "    clear.click(lambda: ([], \"\", None), outputs=[chatbot, msg])\n",
    "    chat_button.click(chat_call, [msg, chatbot, img_input, result_box], [msg, chatbot])\n",
    "    demo.load(fn=refresh, inputs=[img_output], outputs=img_output, show_progress=False, every=5)\n",
    "    \n",
    "    # Bind the product description submit button to the generate_output function\n",
    "    create_button.click(create_call, [img_input, warp_checkbox],outputs=img_output)\n",
    "    upload_button.upload(upload_file, upload_button)  \n",
    "    upload_button_background.upload(upload_file_background, upload_button_background)  \n",
    "    submission_button.click(submission_call, inputs=[submission_text, img_input, chatbot], outputs=result_box)\n",
    "    submission_text.submit(submission_call, inputs=[submission_text, img_input, chatbot], outputs=result_box)\n",
    "\n",
    "    # Include the language and warp selection in the output function\n",
    "    submission_button.click(submission_call, inputs=[submission_text, img_input, chatbot, language_radio], outputs=result_box)\n",
    "    submission_text.submit(submission_call, inputs=[submission_text, img_input, chatbot, language_radio], outputs=result_box)\n",
    "    \n",
    "# Launch the Gradio interface\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "059243a6afb442ccbef314aebe1dc5ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b4f1e8b5afd42c39ddd4dc803da0164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0ced19ed88d34b65b44dd6da8aa38de4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3185e496830042c3bfef1d0530b2c392": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45a850cd644742eab40a243fd1c85d4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1aab42246d14a1db588e7abb1c4001f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0ced19ed88d34b65b44dd6da8aa38de4",
      "value": "‚Äá5/5‚Äá[00:08&lt;00:00,‚Äá‚Äá1.53s/it]"
     }
    },
    "7df5a9eef9e048a6978f8e3d729a5704": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89aa5ec9958343d9966d35581b21a5aa",
       "IPY_MODEL_bbc1b4af122642d2842978091c40806c",
       "IPY_MODEL_45a850cd644742eab40a243fd1c85d4c"
      ],
      "layout": "IPY_MODEL_3185e496830042c3bfef1d0530b2c392"
     }
    },
    "89aa5ec9958343d9966d35581b21a5aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4a39eb9fb6c481ba08bcd9853210849",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f0062dc0f3b94a93bca682fdb7b130dc",
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
     }
    },
    "a1aab42246d14a1db588e7abb1c4001f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbc1b4af122642d2842978091c40806c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_059243a6afb442ccbef314aebe1dc5ee",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b4f1e8b5afd42c39ddd4dc803da0164",
      "value": 5
     }
    },
    "e4a39eb9fb6c481ba08bcd9853210849": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0062dc0f3b94a93bca682fdb7b130dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
