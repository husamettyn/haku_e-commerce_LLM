{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a2BfPX9Z8p1",
    "outputId": "c07720b5-c3fe-4800-d7ae-80af069f4242"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U gradio \n",
    "!pip install pillow-avif-plugin Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaULqLbwaoGo",
    "outputId": "96382dc5-ee47-499a-8e0c-332a1f001be4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/jupyter/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"YOU_LOGIN_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p3mUxxAIb4hw"
   },
   "outputs": [],
   "source": [
    "HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iOcYTep2cMTW"
   },
   "outputs": [],
   "source": [
    "# prompt: create code for clearing torch ram\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "7df5a9eef9e048a6978f8e3d729a5704",
      "89aa5ec9958343d9966d35581b21a5aa",
      "bbc1b4af122642d2842978091c40806c",
      "45a850cd644742eab40a243fd1c85d4c",
      "3185e496830042c3bfef1d0530b2c392",
      "e4a39eb9fb6c481ba08bcd9853210849",
      "f0062dc0f3b94a93bca682fdb7b130dc",
      "059243a6afb442ccbef314aebe1dc5ee",
      "0b4f1e8b5afd42c39ddd4dc803da0164",
      "a1aab42246d14a1db588e7abb1c4001f",
      "0ced19ed88d34b65b44dd6da8aa38de4"
     ]
    },
    "id": "CMVnNWzEZmH6",
    "outputId": "6bc2eba6-d9f3-4be7-d098-4ac549cb85d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb950512ba8149b4bc5ae129c6bab29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29edb37f0f92424cbbf4c06e280ee939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "model_id_2 = \"Trendyol/Trendyol-LLM-7b-chat-v1.8\"\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_id_2,use_fast=False)\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(model_id_2, \n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype= torch.bfloat16,\n",
    "                                             load_in_8bit=True)\n",
    "\n",
    "sampling_params_2 = dict(do_sample=True, temperature=0.3, top_k=50, top_p=0.9)\n",
    "\n",
    "pipe_2 = pipeline(\"text-generation\", \n",
    "                model=model_2, \n",
    "                tokenizer=tokenizer_2,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype= torch.bfloat16,\n",
    "                max_new_tokens=1024, \n",
    "                return_full_text=True,\n",
    "                repetition_penalty=1.1\n",
    "               )\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT_2 = \"Bir e ticaret sitesi i√ßin a√ßƒ±klama √ºreten bir yapay zeka modelisin \\n\"\n",
    "\n",
    "TEMPLATE_2 = (\n",
    "    \"[INST] {system_prompt}\\n\\n\"\n",
    "    \"{instruction} [/INST]\"\n",
    ")\n",
    "\n",
    "def generate_prompt_2(instruction, system_prompt=DEFAULT_SYSTEM_PROMPT_2):\n",
    "    return TEMPLATE_2.format_map({'instruction': instruction,'system_prompt': system_prompt})\n",
    "\n",
    "def generate_output_2(user_query, sys_prompt=DEFAULT_SYSTEM_PROMPT_2):\n",
    "    prompt = generate_prompt_2(user_query, sys_prompt)\n",
    "    outputs = pipe_2(prompt,\n",
    "               **sampling_params_2\n",
    "              )\n",
    "    return outputs[0][\"generated_text\"].split(\"[/INST]\")[-1]\n",
    "\n",
    "#generate_output_2(\"Bir e ticaret sitesi i√ßin a√ßƒ±klama √ºreten bir yapay zeka modelisin. Senden bir e ticaret sitesindeki earphone √ºr√ºn√º i√ßin a√ßƒ±klama yazmanƒ± istiyorum. A√ßƒ±klamada kullanacaƒüƒ±n √∂zellikler:{'color':'black','sound':'good'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "iF5-JS1LtsbW",
    "outputId": "d8c02ebe-1e9e-40a7-ccf0-9916b6cccb9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import gradio as gr\n",
    "import time\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "from skimage import io  \n",
    "import shutil   \n",
    "import json  # Import json to work with JSON files\n",
    "import urllib.request as request  # Import urllib to make HTTP requests\n",
    "\n",
    "def queue_prompt(prompt):\n",
    "    p = {\"prompt\": prompt}\n",
    "    data = json.dumps(p).encode('utf-8')\n",
    "    req = request.Request(\"http://127.0.0.1:3000/prompt\", data=data, headers={'Content-Type': 'application/json'})\n",
    "    response = request.urlopen(req)  # Make the HTTP request\n",
    "    print(response.read().decode())  # Print the response\n",
    "    \n",
    "def clear_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)  # Remove file\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove folder\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "                \n",
    "def process_input(input_text):\n",
    "    # Define the tags\n",
    "    header_start = \"<|start_header_id|>\"\n",
    "    header_end = \"<|end_header_id|>\"\n",
    "    eot_id = \"<|eot_id|>\"\n",
    "\n",
    "    # Find the last occurrence of the header tags\n",
    "    header_start_idx = input_text.rfind(header_start) + len(header_start)\n",
    "    header_end_idx = input_text.rfind(header_end)\n",
    "    bot_name = input_text[header_start_idx:header_end_idx].strip()\n",
    "\n",
    "    # Find the message content between the last end header and the eot tag\n",
    "    message_start_idx = header_end_idx + len(header_end)\n",
    "\n",
    "    # Check if the eot_id exists and if the string ends with it\n",
    "    if input_text.endswith(eot_id):\n",
    "        message_end_idx = input_text.rfind(eot_id)\n",
    "    else:\n",
    "        message_end_idx = len(input_text)\n",
    "\n",
    "    message = input_text[message_start_idx:message_end_idx].strip()\n",
    "\n",
    "    return f\"{message}\"\n",
    "\n",
    "def create_response(uploaded_image,content,max_new_tokens=200,process_input_boolean=True):\n",
    "    # Example input_text processing (replace with your actual processor code)\n",
    "    input_text = processor.apply_chat_template(content, add_generation_prompt=True) #works\n",
    "    inputs = processor(uploaded_image, input_text, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    bot_message = processor.decode(output[0])\n",
    "    if process_input_boolean:\n",
    "        bot_message = process_input(bot_message)\n",
    "    return bot_message\n",
    "\n",
    "def get_product_name(explanation,uploaded_image):\n",
    "    message = f\"\"\"Based on the provided product description and image: '{explanation}' explain the product at image.\"\"\"\n",
    "    content = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": message}\n",
    "    ]}\n",
    "    ]\n",
    "    name_answer = create_response(uploaded_image,content,max_new_tokens=100)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": name_answer}\n",
    "    ]})\n",
    "    message_2 = \"What is the name of the product?\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_2}\n",
    "    ]})\n",
    "    name_answer_2 = create_response(uploaded_image,content,max_new_tokens=30)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": name_answer_2}\n",
    "    ]})\n",
    "\n",
    "    return name_answer_2\n",
    "\n",
    "def get_target_audience(product_name,uploaded_image):\n",
    "    message = f\"\"\"Can you provide detailed information about {product_name} ?\"\"\"\n",
    "\n",
    "    content = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": message}\n",
    "    ]}\n",
    "    ]\n",
    "    target_answer = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": target_answer}\n",
    "    ]})\n",
    "    message_2 = f\"\"\"Based on the information about {product_name} and image, which type of people can be our target audience and why?\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_2}\n",
    "    ]})\n",
    "    target_answer_2 = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": target_answer_2}\n",
    "    ]})\n",
    "    message_3 = f\"\"\"I want one target audience that includes all of these.\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_3}\n",
    "    ]})\n",
    "    target_answer_3 = create_response(uploaded_image,content,max_new_tokens=50)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": target_answer_3}\n",
    "    ]})\n",
    "\n",
    "    return target_answer_3\n",
    "\n",
    "def get_strong_message(product_name,uploaded_image,target_audience,all_features):\n",
    "    message = f\"\"\"Please provide a comprehensive overview of {product_name} at the image. Do not add information that you are not sure.\"\"\"\n",
    "    content = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": message}\n",
    "    ]}\n",
    "    ]\n",
    "    sm_answer = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": sm_answer}\n",
    "    ]})\n",
    "    message_3 = f\"\"\"Create a e commerce explanation for {target_audience} using these features {all_features} and previous information. Do not make longer than 200 words.\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_3}\n",
    "    ]})\n",
    "    sm_answer_3 = create_response(uploaded_image,content)\n",
    "\n",
    "    return sm_answer_3\n",
    "\n",
    "def get_all_features(product_name, uploaded_image):\n",
    "    message = f\"\"\"Please provide details about features of {product_name} at the image.Do not add any feature that is visible at the image. \"\"\"\n",
    "    content = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": message}\n",
    "    ]}\n",
    "    ]\n",
    "    f_answer = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": f_answer}\n",
    "    ]})\n",
    "    message_2 = f\"\"\"Given the detailed information about {product_name} what is the features about the product at the image?Do not add any feature that is not provided. \"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_2}\n",
    "    ]})\n",
    "    f_answer_2 = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": f_answer_2}\n",
    "    ]})\n",
    "    message_3 = \"\"\"Create a dictionary for the features of the product in this format:{'color':'black',...}.Do not add any feature that you are not sure. Only provide the format.\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_3}\n",
    "    ]})\n",
    "    f_answer_3 = create_response(uploaded_image,content)\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": f_answer_3}\n",
    "    ]})\n",
    "    all_features = f_answer_3\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "def get_usefull_features(product_name,target_audience,uploaded_image,chat_history):\n",
    "    message_uf = f\"\"\"\n",
    "    Think like a {target_audience} user that uses {product_name} in the image which features would you like to have in your {product_name}.\n",
    "    \"\"\"\n",
    "    content = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": message_uf}\n",
    "    ]}\n",
    "    ]\n",
    "    answer_uf = create_response(uploaded_image,content)\n",
    "    chat_history.append((message_uf, answer_uf))\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": answer_uf}\n",
    "    ]})\n",
    "    message_2_uf = f\"\"\"Create list of the features that you provided before.\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_2_uf}\n",
    "    ]})\n",
    "    answer_2_uf = create_response(uploaded_image,content)\n",
    "    chat_history.append((message_2_uf, answer_2_uf))\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": answer_2_uf}\n",
    "    ]})\n",
    "    message_3_uf = f\"\"\"Create list of the features important for you in this format:['feature1','feature2'...].Only provide the format.\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_3_uf}\n",
    "    ]})\n",
    "    answer_3_uf = create_response(uploaded_image,content,max_new_tokens=100)\n",
    "    chat_history.append((message_3_uf, answer_3_uf))\n",
    "\n",
    "    return answer_3_uf\n",
    "\n",
    "def get_ocr(product_name,uploaded_image,chat_history):\n",
    "    \n",
    "    message_ocr = f\"\"\"Provide every text that is written at the image.\"\"\"\n",
    "    content = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": message_ocr}\n",
    "    ]}\n",
    "    ]\n",
    "    answer_ocr = create_response(uploaded_image,content)\n",
    "    chat_history.append((message_ocr, answer_ocr))\n",
    "    content.append({\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": answer_ocr}\n",
    "    ]})\n",
    "    message_2_ocr = f\"\"\"Provide every text that is written at the image.\"\"\"\n",
    "    content.append({\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": message_2_ocr}\n",
    "    ]})\n",
    "    answer_2_ocr = create_response(uploaded_image,content)\n",
    "    chat_history.append((message_2_ocr, answer_2_ocr))\n",
    "\n",
    "    return answer_2_ocr\n",
    "\n",
    "def description_generate(message, uploaded_image):\n",
    "    product_name = get_product_name(message, uploaded_image)\n",
    "    target_audience = get_target_audience(product_name,uploaded_image)\n",
    "    all_features = get_all_features(product_name, uploaded_image)\n",
    "    strong_message = get_strong_message(product_name, uploaded_image,target_audience,all_features)\n",
    "    return strong_message\n",
    "\n",
    "def chat_call(message, chat_history, uploaded_image, result_text):\n",
    "    try:\n",
    "        # Eƒüer g√∂rsel veya a√ßƒ±klama eksikse\n",
    "        if uploaded_image is None or (result_text == \"\" and len(chat_history) == 0):\n",
    "            gr.Warning(\"√úr√ºn G√∂rseli ya da A√ßƒ±klama Eksik.\")\n",
    "        else:\n",
    "            history_openai_format = []\n",
    "            if len(chat_history)==0:\n",
    "                history_openai_format.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": 'Provide explanation for e commerce store'},  # √úr√ºn a√ßƒ±klamasƒ±\n",
    "                    ]\n",
    "                })\n",
    "                history_openai_format.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": result_text}]})\n",
    "                chat_history.append(('Provide explanation for e commerce store', result_text))\n",
    "            else:\n",
    "                history_openai_format.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": 'Provide explanation for e commerce store'},  # √úr√ºn a√ßƒ±klamasƒ±\n",
    "                    ]\n",
    "                })\n",
    "                history_openai_format.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": result_text}]})\n",
    "            if len(chat_history)>4:\n",
    "                recent_history = chat_history[-3:]\n",
    "                for human, assistant in recent_history:\n",
    "                    history_openai_format.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": human}]})\n",
    "                    history_openai_format.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant}]})\n",
    "                history_openai_format.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]})    \n",
    "                target_answer = create_response(uploaded_image, history_openai_format)\n",
    "                chat_history.append((message, target_answer))  # Kullanƒ±cƒ±nƒ±n mesajƒ±nƒ± chat_history'ye ekliyoruz\n",
    "            else:\n",
    "                for human, assistant in chat_history:\n",
    "                    history_openai_format.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": human}]})\n",
    "                    history_openai_format.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant}]})\n",
    "                history_openai_format.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": message}]}) \n",
    "                target_answer = create_response(uploaded_image, history_openai_format) \n",
    "                chat_history.append((message, target_answer))  # Kullanƒ±cƒ±nƒ±n mesajƒ±nƒ± chat_history'ye ekliyoruz\n",
    "                \n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred: {str(e)}\"\n",
    "        chat_history.append((message, error_message))\n",
    "\n",
    "    return \"\", chat_history\n",
    "\n",
    "def submission_call(message, uploaded_image,chat_history,language):\n",
    "    try:\n",
    "        # If no image is uploaded\n",
    "        if uploaded_image is None or message == \"\":\n",
    "            gr.Warning(\"√úr√ºn G√∂rseli ya da A√ßƒ±klama Eksik.\")\n",
    "        else:\n",
    "            if language == \"English\":\n",
    "                product_name = get_product_name(message, uploaded_image)\n",
    "                all_features = get_all_features(message, uploaded_image)\n",
    "                target_audience = get_target_audience(product_name,uploaded_image)\n",
    "                strong_message = get_strong_message(product_name,uploaded_image,target_audience,all_features)\n",
    "                return strong_message\n",
    "            else:\n",
    "                product_name = get_product_name(message, uploaded_image)\n",
    "                all_features = get_all_features(message, uploaded_image)\n",
    "                return generate_output_2(f\"A√ßƒ±klamada kullanacaƒüƒ±n √∂zellikler:{all_features}.Sen bir e ticaret sitesi i√ßin a√ßƒ±klama √ºreten bir yapay zeka modelisin. Senden bir e ticaret sitesindeki {product_name} √ºr√ºn√º i√ßin m√º≈üterilere hitap eden bir a√ßƒ±klama yazmanƒ± istiyorum. \")\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "def create_call(img_output):\n",
    "    def queue_prompt(prompt):\n",
    "        p = {\"prompt\": prompt}\n",
    "        data = json.dumps(p).encode('utf-8')\n",
    "        req = request.Request(\"http://127.0.0.1:3000/prompt\", data=data, headers={'Content-Type': 'application/json'})\n",
    "        response = request.urlopen(req)  # Make the HTTP request\n",
    "    # Function to get the latest file from a directory\n",
    "    def get_latest_file_in_folder(folder_path):\n",
    "        files = os.listdir(folder_path)\n",
    "        if files:\n",
    "            # Get the latest file based on modification time\n",
    "            return sorted(files, key=lambda x: os.path.getmtime(os.path.join(folder_path, x)))[0]\n",
    "        return None\n",
    "    with open('son_api.json', 'r') as file:\n",
    "        prompt = json.load(file)  # Load JSON content from the file\n",
    "        # Get the latest file names in each folder\n",
    "    queue_prompt(prompt)\n",
    "    # Define the folder paths\n",
    "    main_folder = \"./ComfyUI/input/main\"\n",
    "    background_folder = \"./ComfyUI/input/background\"\n",
    "    \n",
    "    main_image = get_latest_file_in_folder(main_folder)  # Get the latest file from the main folder\n",
    "    background_image = get_latest_file_in_folder(background_folder)  # Get the latest file from the background folder\n",
    "\n",
    "    # Update the prompt dictionary with the relative paths of the files\n",
    "    if main_image is not None:\n",
    "        prompt['10']['inputs']['image'] = f\"main/{main_image}\"  # Update image input with the latest main folder file \n",
    "    else:\n",
    "        print(\"No image found in the main folder.\")\n",
    "\n",
    "    if background_image is not None:\n",
    "        prompt['39']['inputs']['image'] = f\"background/{background_image}\"  # Update background image input with only the background path\n",
    "    else:\n",
    "        print(\"No image found in the background folder.\")\n",
    "    queue_prompt(prompt)\n",
    "    OUTPUT_FOLDER = \"./ComfyUI/output\"\n",
    "    clear_folder(OUTPUT_FOLDER) \n",
    "def refresh(result_box):\n",
    "    OUTPUT_FOLDER = \"./ComfyUI/output\"\n",
    "    for file_name in os.listdir(OUTPUT_FOLDER):\n",
    "        # Check if the file is an image (you can add more extensions if needed)\n",
    "        if file_name.endswith(('.png', '.jpg', '.jpeg', '.webp', '.gif')):\n",
    "            image_path = os.path.join(OUTPUT_FOLDER, file_name)\n",
    "            print(f\"Image found: {image_path}\")\n",
    "            return image_path\n",
    "    \n",
    "def upload_file(file):    \n",
    "    UPLOAD_FOLDER = \"./ComfyUI/input/main\" \n",
    "    clear_folder(UPLOAD_FOLDER) \n",
    "    if not os.path.exists(UPLOAD_FOLDER):    \n",
    "        os.mkdir(UPLOAD_FOLDER)    \n",
    "    shutil.copy(file, UPLOAD_FOLDER)    \n",
    "    gr.Info(\"Main image uploaded!!!\")\n",
    "    \n",
    "def upload_file_background(file):    \n",
    "    UPLOAD_FOLDER = \"./ComfyUI/input/background\"\n",
    "    clear_folder(UPLOAD_FOLDER) \n",
    "    if not os.path.exists(UPLOAD_FOLDER):    \n",
    "        os.mkdir(UPLOAD_FOLDER)    \n",
    "    shutil.copy(file, UPLOAD_FOLDER)    \n",
    "    gr.Info(\"Background uploaded!!!\")  \n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():  # Create a row to position components horizontally\n",
    "        # Left Panel\n",
    "        with gr.Column(scale=2):  # Image upload button on the left\n",
    "            img_input = gr.Image(label=\"√úr√ºn G√∂rseli Ekleyiniz.\")  # Image input component\n",
    "            chatbot = gr.Chatbot()\n",
    "            msg = gr.Textbox(placeholder=\"A√ßƒ±klamayƒ± deƒüi≈ütirmek i√ßin chatbot ile konu≈üabilirsin.\", label=\"Chatbot\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    chat_button = gr.Button(\"G√∂nder\")\n",
    "                with gr.Column():\n",
    "                    # Clear button with a trash can emoji in the text\n",
    "                    clear = gr.Button(\"Sohbeti Temizle üóëÔ∏è\")\n",
    "\n",
    "        # Right Panel\n",
    "        with gr.Column(scale=4):\n",
    "            submission_text = gr.Textbox(placeholder=\"√úr√ºn A√ßƒ±klamasƒ±\", label=\"√úr√ºn√ºn√ºz hakkƒ±nda kƒ±sa bir a√ßƒ±klama giriniz.\")\n",
    "            submission_button = gr.Button(\"Olu≈ütur\")\n",
    "            result_box = gr.Textbox(label=\"√áƒ±ktƒ±\", interactive=False)\n",
    "            upload_button = gr.UploadButton(\"Upload main image\")    \n",
    "            upload_button_background = gr.UploadButton(\"Upload background image\")\n",
    "            create_button = gr.Button('Create new image')\n",
    "            img_output = gr.Image(type=\"filepath\", interactive=False, label=\"Output Image\")\n",
    "            \n",
    "            # Adding radio button for language selection\n",
    "            language_radio = gr.Radio(\n",
    "                choices=[\"English\", \"T√ºrk√ße\"], \n",
    "                label=\"Dil Se√ßimi\", \n",
    "                value=\"English\"  # Default selected value\n",
    "            )\n",
    "            \n",
    "            # Adding checkbox to choose whether to apply warp\n",
    "            warp_checkbox = gr.Checkbox(label=\"Warp Uygula\", value=False)  # Default unchecked\n",
    "            \n",
    "    # Bind both the submit event on the text box and the button click to the chat_call function\n",
    "    msg.submit(chat_call, [msg, chatbot, img_input, result_box], [msg, chatbot])\n",
    "    clear.click(lambda: ([], \"\", None), outputs=[chatbot, msg])\n",
    "    chat_button.click(chat_call, [msg, chatbot, img_input, result_box], [msg, chatbot])\n",
    "    demo.load(fn=refresh, inputs=[img_output], outputs=img_output, show_progress=False, every=5)\n",
    "    \n",
    "    # Bind the product description submit button to the generate_output function\n",
    "    create_button.click(create_call, [img_input],outputs=result_box)\n",
    "    upload_button.upload(upload_file, upload_button)  \n",
    "    upload_button_background.upload(upload_file_background, upload_button_background)  \n",
    "    submission_button.click(submission_call, inputs=[submission_text, img_input, chatbot], outputs=result_box)\n",
    "    submission_text.submit(submission_call, inputs=[submission_text, img_input, chatbot], outputs=result_box)\n",
    "\n",
    "    # Include the language and warp selection in the output function\n",
    "    submission_button.click(submission_call, inputs=[submission_text, img_input, chatbot, language_radio], outputs=result_box)\n",
    "    submission_text.submit(submission_call, inputs=[submission_text, img_input, chatbot, language_radio], outputs=result_box)\n",
    "    \n",
    "# Launch the Gradio interface\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "059243a6afb442ccbef314aebe1dc5ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b4f1e8b5afd42c39ddd4dc803da0164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0ced19ed88d34b65b44dd6da8aa38de4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3185e496830042c3bfef1d0530b2c392": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45a850cd644742eab40a243fd1c85d4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1aab42246d14a1db588e7abb1c4001f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0ced19ed88d34b65b44dd6da8aa38de4",
      "value": "‚Äá5/5‚Äá[00:08&lt;00:00,‚Äá‚Äá1.53s/it]"
     }
    },
    "7df5a9eef9e048a6978f8e3d729a5704": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89aa5ec9958343d9966d35581b21a5aa",
       "IPY_MODEL_bbc1b4af122642d2842978091c40806c",
       "IPY_MODEL_45a850cd644742eab40a243fd1c85d4c"
      ],
      "layout": "IPY_MODEL_3185e496830042c3bfef1d0530b2c392"
     }
    },
    "89aa5ec9958343d9966d35581b21a5aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4a39eb9fb6c481ba08bcd9853210849",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f0062dc0f3b94a93bca682fdb7b130dc",
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
     }
    },
    "a1aab42246d14a1db588e7abb1c4001f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbc1b4af122642d2842978091c40806c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_059243a6afb442ccbef314aebe1dc5ee",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b4f1e8b5afd42c39ddd4dc803da0164",
      "value": 5
     }
    },
    "e4a39eb9fb6c481ba08bcd9853210849": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0062dc0f3b94a93bca682fdb7b130dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
